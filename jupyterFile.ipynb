{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import messagebox\n",
    "from tkinter import *\n",
    "from tkinter import simpledialog\n",
    "import tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "main = tkinter.Tk()\n",
    "main.title(\"EXTENSION OF THE LEXICON ALGORITHM FOR SARCASM DETECTION\") #designing main screen\n",
    "main.geometry(\"1300x1200\")\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "global filename\n",
    "global dataset\n",
    "global process\n",
    "global sarcastic\n",
    "global sentiment\n",
    "\n",
    "def checkSarcasm(sentence):\n",
    "    pos = []\n",
    "    neg = []\n",
    "    neu = []\n",
    "    arr = sentence.split(' ')\n",
    "    for i in range(len(arr)):\n",
    "        word = arr[i].strip()\n",
    "        if word == 'smilingfacewithhearteyes':\n",
    "            word = 'excellent'\n",
    "        if word == 'loudlycryingface':\n",
    "            word = 'bad'\n",
    "        if word == 'winkingfacewithtongue':\n",
    "            word = 'happy'    \n",
    "        if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "            pos.append(word)\n",
    "        elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "            neg.append(word)\n",
    "        else:\n",
    "            neu.append(word)\n",
    "    return pos,neg,neu    \n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens) #here upto for word based\n",
    "    return tokens\n",
    "\n",
    "def upload():\n",
    "    global filename\n",
    "    global dataset\n",
    "    dataset = []\n",
    "    filename = filedialog.askopenfilename(initialdir=\"dataset\")\n",
    "    text.delete('1.0', END)\n",
    "    text.insert(END,filename+\" loaded\\n\");\n",
    "    train = pd.read_csv(filename,encoding='utf8',sep='\\t')\n",
    "    count = 0\n",
    "    for i in range(len(train)):\n",
    "        tweet = train.get_value(i,0,takeable = True)\n",
    "        print(tweet)\n",
    "        if str(tweet) != 'nan':\n",
    "            tweet = tweet.lower()\n",
    "        icon = train.get_value(i,1,takeable = True)\n",
    "        if str(icon) != 'nan':\n",
    "            icon = UNICODE_EMOJI[icon.strip()]\n",
    "            icon = ''.join(re.sub('[^A-Za-z\\s]+', '', icon))\n",
    "            icon = icon.lower()\n",
    "        else:\n",
    "            icon = ''\n",
    "        msg = ''\n",
    "        if str(tweet) != 'nan':\n",
    "            arr = tweet.split(\" \")\n",
    "            for k in range(len(arr)):\n",
    "                word = arr[k].strip()\n",
    "                if len(word) > 2:\n",
    "                    msg+=word+\" \"\n",
    "        textdata = msg.strip()+\" \"+icon\n",
    "        #print(textdata)\n",
    "        dataset.append(textdata)\n",
    "       \n",
    "    text.insert(END,'Total tweets found in dataset is : '+str(len(dataset)))\n",
    "\n",
    "def Preprocessing():\n",
    "    text.delete('1.0', END)\n",
    "    global process\n",
    "    process = []\n",
    "    text.insert(END,'Messages after preprocessing and removing stopwords\\n')\n",
    "    text.insert(END,'====================================================================================\\n')\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        sentence = dataset[i]\n",
    "        sentence = sentence.lower()\n",
    "        sentence = clean_doc(sentence)\n",
    "        text.insert(END,sentence+'\\n')\n",
    "        process.append(sentence)\n",
    "                \n",
    "def firstAlgorithm():\n",
    "    text.delete('1.0', END)\n",
    "    global sarcastic\n",
    "    sarcastic = []\n",
    "    for i in range(len(process)):\n",
    "        sentence = process[i]\n",
    "        if sentence == 'smilingfacewithhearteyes':\n",
    "            sentence = 'excellent'\n",
    "        if sentence == 'loudlycryingface':\n",
    "            sentence = 'bad'\n",
    "        if sentence == 'winkingfacewithtongue':\n",
    "            sentence = 'happy'   \n",
    "        sentiment_dict = sid.polarity_scores(sentence)\n",
    "        negative_polarity = sentiment_dict['neg']\n",
    "        positive_polarity = sentiment_dict['pos']\n",
    "        neutral_polarity = sentiment_dict['neu']\n",
    "        compound = sentiment_dict['compound']\n",
    "        result = ''\n",
    "        if compound >= 0.1 :\n",
    "            result = 'Positive' \n",
    "        elif compound <= -0.1:\n",
    "            result = 'Negative' \n",
    "        else :\n",
    "            result = 'Neutral'\n",
    "        if result =='Positive' or result == 'Neutral':\n",
    "            pos,neg,neu = checkSarcasm(sentence)\n",
    "            if len(neg) > 0:\n",
    "                sarcastic.append(\"Sarcastic\")\n",
    "            else:\n",
    "                sarcastic.append(\"Non Sarcastic\")\n",
    "        else:\n",
    "            sarcastic.append(\"Non Sarcastic\")\n",
    "        text.insert(END,'Tweets : '+dataset[i]+\"\\n\")\n",
    "        text.insert(END,'Positive Polarity : '+str(positive_polarity)+\"\\n\")\n",
    "        text.insert(END,'Negative Polarity : '+str(negative_polarity)+\"\\n\")\n",
    "        text.insert(END,'Neutral Polarity  : '+str(neutral_polarity)+\"\\n\")\n",
    "        text.insert(END,'Result : '+sarcastic[i]+\"\\n\")\n",
    "        text.insert(END,'====================================================================================\\n')\n",
    "            \n",
    "def secondAlgorithm():\n",
    "    global sentiment\n",
    "    sentiment = []\n",
    "    text.delete('1.0', END)\n",
    "    for i in range(len(process)):\n",
    "        sentence = process[i]\n",
    "        if sentence == 'smilingfacewithhearteyes':\n",
    "            sentence = 'excellent'\n",
    "        if sentence == 'loudlycryingface':\n",
    "            sentence = 'bad'\n",
    "        if sentence == 'winkingfacewithtongue':\n",
    "            sentence = 'happy'   \n",
    "        sentiment_dict = sid.polarity_scores(sentence)\n",
    "        negative_polarity = sentiment_dict['neg']\n",
    "        positive_polarity = sentiment_dict['pos']\n",
    "        neutral_polarity = sentiment_dict['neu']\n",
    "        compound = sentiment_dict['compound']\n",
    "        result = ''\n",
    "        if compound >= 0.1 :\n",
    "            result = 'Positive'\n",
    "            sentiment.append(result)\n",
    "        elif compound <= -0.1:\n",
    "            result = 'Negative'\n",
    "            sentiment.append(result)\n",
    "        else :\n",
    "            result = 'Neutral'\n",
    "            sentiment.append(result)\n",
    "        sar = ''    \n",
    "        if result =='Positive' or result == 'Neutral':\n",
    "            pos,neg,neu = checkSarcasm(sentence)\n",
    "            if len(neg) > 0:\n",
    "                sar = \"Sarcastic\"\n",
    "            else:\n",
    "                sar = \"Non Sarcastic\"\n",
    "        else:\n",
    "            sar = \"Non Sarcastic\"\n",
    "            \n",
    "        text.insert(END,'Tweets : '+dataset[i]+\"\\n\")\n",
    "        text.insert(END,'Positive Polarity : '+str(positive_polarity)+\"\\n\")\n",
    "        text.insert(END,'Negative Polarity : '+str(negative_polarity)+\"\\n\")\n",
    "        text.insert(END,'Neutral Polarity  : '+str(neutral_polarity)+\"\\n\")\n",
    "        text.insert(END,'Result : '+sar+\"\\n\")\n",
    "        text.insert(END,'Sentiment Prediction : '+result+'\\n')\n",
    "        text.insert(END,'====================================================================================\\n')\n",
    "    \n",
    "\n",
    "def sarcasticGraph():\n",
    "    sar = 0\n",
    "    non_sar = 0\n",
    "    for i in range(len(sarcastic)):\n",
    "        if sarcastic[i] == \"Sarcastic\":\n",
    "            sar = sar + 1\n",
    "        if sarcastic[i] == \"Non Sarcastic\":\n",
    "            non_sar = non_sar + 1\n",
    "    height = [sar,non_sar]\n",
    "    bars = ('Sarcastic','Non Sarcastic')\n",
    "    y_pos = np.arange(len(bars))\n",
    "    plt.bar(y_pos, height)\n",
    "    plt.xticks(y_pos, bars)\n",
    "    plt.show()\n",
    "\n",
    "def sentimentGraph():\n",
    "  label_X = []\n",
    "  category_X = []\n",
    "  pos = 0\n",
    "  neg = 0\n",
    "  neu = 0\n",
    "  for i in range(len(sentiment)):\n",
    "      if sentiment[i] == 'Positive':\n",
    "          pos = pos + 1\n",
    "      if sentiment[i] == 'Negative':\n",
    "          neg = neg + 1\n",
    "      if sentiment[i] == 'Neutral':\n",
    "          neu = neu + 1    \n",
    "  label_X.append('Positive')\n",
    "  label_X.append('Negative')\n",
    "  label_X.append('Neutral')\n",
    "  category_X.append(pos)\n",
    "  category_X.append(neg)\n",
    "  category_X.append(neu)\n",
    "\n",
    "  plt.pie(category_X,labels=label_X,autopct='%1.1f%%')\n",
    "  plt.title('Sentiment Graph')\n",
    "  plt.axis('equal')\n",
    "  plt.show()\n",
    "\n",
    "font = ('times', 16, 'bold')\n",
    "title = Label(main, text='EXTENSION OF THE LEXICON ALGORITHM FOR SARCASM DETECTION')\n",
    "title.config(bg='LightGoldenrod1', fg='medium orchid')  \n",
    "title.config(font=font)           \n",
    "title.config(height=3, width=120)       \n",
    "title.place(x=0,y=5)\n",
    "\n",
    "font1 = ('times', 12, 'bold')\n",
    "text=Text(main,height=30,width=100)\n",
    "scroll=Scrollbar(text)\n",
    "text.configure(yscrollcommand=scroll.set)\n",
    "text.place(x=400,y=100)\n",
    "text.config(font=font1)\n",
    "\n",
    "\n",
    "font1 = ('times', 12, 'bold')\n",
    "uploadButton = Button(main, text=\"Upload Social Network Dataset\", command=upload)\n",
    "uploadButton.place(x=50,y=100)\n",
    "uploadButton.config(font=font1)  \n",
    "\n",
    "preButton = Button(main, text=\"Preprocess Dataset\", command=Preprocessing)\n",
    "preButton.place(x=50,y=150)\n",
    "preButton.config(font=font1) \n",
    "\n",
    "firstButton = Button(main, text=\"Run First System Lexicon + Polarity Computation\", command=firstAlgorithm)\n",
    "firstButton.place(x=50,y=200)\n",
    "firstButton.config(font=font1) \n",
    "\n",
    "secondButton = Button(main, text=\"Second System Lexicon + Sentiment Prediction\", command=secondAlgorithm)\n",
    "secondButton.place(x=50,y=250)\n",
    "secondButton.config(font=font1) \n",
    "\n",
    "graphButton = Button(main, text=\"Sentiments Graph\", command=sentimentGraph)\n",
    "graphButton.place(x=50,y=300)\n",
    "graphButton.config(font=font1)\n",
    "\n",
    "gButton = Button(main, text=\"Sarcastic Graph\", command=sarcasticGraph)\n",
    "gButton.place(x=50,y=350)\n",
    "gButton.config(font=font1)\n",
    "\n",
    "\n",
    "main.config(bg='OliveDrab2')\n",
    "main.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
